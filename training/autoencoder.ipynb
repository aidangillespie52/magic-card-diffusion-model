{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the current working directory (where the notebook is located)\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory to sys.path\n",
    "sys.path.append('../models')\n",
    "sys.path.append('../scripts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from random import randint\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from autoencoder import ResNetAutoencoder\n",
    "from mtg_cards_dataset import MTGCardsDataset\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 1\n",
    "LOGGING_INTERVAL = 150\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Displaying Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preview Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataset, num_samples=10, cols=4):\n",
    "    \"\"\"Plots some samples from the dataset.\"\"\"\n",
    "    transform_to_pil = transforms.ToPILImage()\n",
    "    rows = (num_samples + cols - 1) // cols  # Calculate rows needed\n",
    "    plt.figure(figsize=(cols * 4, rows * 4))  # Adjust figure size\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        img, _ = dataset[i]  # Extract image and label (label is ignored here)\n",
    "        img = transform_to_pil(img)  # Convert tensor to PIL image for plotting\n",
    "        \n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')  # Hide axes for cleaner visualization\n",
    "    \n",
    "    plt.tight_layout()  # Optimize spacing between images\n",
    "    plt.show()\n",
    "\n",
    "def show_tensor_image(image):\n",
    "    reverse_transforms = transforms.Compose([\n",
    "        transforms.Lambda(lambda t: (t + 1) / 2),\n",
    "        transforms.Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC\n",
    "        transforms.Lambda(lambda t: t * 255.),\n",
    "        transforms.Lambda(lambda t: t.numpy().astype(np.uint8)),\n",
    "        transforms.ToPILImage(),\n",
    "    ])\n",
    "\n",
    "    # Take first image of batch\n",
    "    if len(image.shape) == 4:\n",
    "        image = image[0, :, :, :] \n",
    "    plt.imshow(reverse_transforms(image))\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()          # Convert images to tensors\n",
    "])\n",
    "\n",
    "data_dir = \"../data\"\n",
    "preview_dataset = MTGCardsDataset(data_dir, transform=transform)\n",
    "\n",
    "show_images(preview_dataset, num_samples=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),          # Convert images to tensors\n",
    "    transforms.Lambda(lambda t: (t * 2) - 1) # Scale between [-1, 1] \n",
    "])\n",
    "\n",
    "dataset = MTGCardsDataset(data_dir, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNetAutoencoder()\n",
    "\n",
    "if LOAD_MODEL:\n",
    "    model.load_state_dict(torch.load('trained_models/autoencoder.pth'))\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    epoch_loss = 0.0\n",
    "    model.train()\n",
    "    \n",
    "    for batch_idx, (features, _) in enumerate(dataloader):\n",
    "        features = features.to(device)\n",
    "\n",
    "        optimizer.zero_grad()  # Zero out the gradients at the beginning of the loop\n",
    "        logits = model(features)  # Forward pass\n",
    "        loss = loss_fn(logits, features)  # Compute loss\n",
    "\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if batch_idx % LOGGING_INTERVAL == 0:\n",
    "            print(f\"Epoch {epoch+1}, Batch {batch_idx+1}, Loss: {loss.item()}\")\n",
    "            idx = randint(0, len(dataset))\n",
    "            img, _ = dataset[idx]\n",
    "            model.test(img)\n",
    "\n",
    "    # Save model if required\n",
    "    if SAVE_MODEL:\n",
    "        torch.save(model.state_dict(), 'trained_models/autoencoder.pth')\n",
    "\n",
    "model = ResNetAutoencoder()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
